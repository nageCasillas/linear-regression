{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54bb5fb-88b0-4aff-be8d-70d6b51f09c2",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "## Contents\n",
    "- Description  \n",
    "- Errors\n",
    "- Cost Function\n",
    "- Convergence Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee8155-f225-498d-a51e-4180b443e134",
   "metadata": {},
   "source": [
    "## Description  \n",
    "Simple linear regression is a statistical method used to model the relationship between two variables: one independent variable (predictor) and one dependent variable (outcome). The goal is to find a linear equation, \\( y = mx + b \\), where \\( y \\) is the dependent variable, \\( x \\) is the independent variable, \\( m \\) is the slope (rate of change), and \\( b \\) is the intercept (value of \\( y \\) when \\( x = 0 \\))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636e147-f52d-4708-831a-96ed753334cc",
   "metadata": {},
   "source": [
    "## Errors\n",
    "Simple linear regression trys to make a best fit line with a minimium error.  **errors** represent the difference between the actual observed values\n",
    "and the values predicted by the regression model. The error for each data point is calculated as:\n",
    "$$ E_i = y_i - \\hat{y}_i $$\n",
    "\n",
    "Where:\n",
    "- $E_i$  is the error for the \\( i \\)-th observation,\n",
    "- $y_i$  is the actual observed value of the dependent variable,\n",
    "- $\\hat{y}_i$  is the predicted value from the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e931e6-7934-4e7f-8a2b-c11a34904c18",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "The cost function measures the difference between the predicted values of the model and the actual target values. By minimizing this cost function, we can determine the optimal values for the modelâ€™s parameters and improve its performance.\n",
    "\n",
    "The Mean Squared Error (MSE) is given by:\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "Where:\n",
    "- $y_i$ is the actual value\n",
    "- $\\hat{y}_i$ is the predicted value\n",
    "- $n$ is the total number of data points\n",
    "\n",
    "Reference: https://medium.com/@yennhi95zz/3-understanding-the-cost-function-in-linear-regression-for-machine-learning-beginners-ec9edeecbdde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12abd447-c326-4846-b2fb-15b5efbee898",
   "metadata": {},
   "source": [
    "## Convergence Algorithm\n",
    "Gradient Descent is an iterative optimization algorithm that tries to find the optimum value (Minimum/Maximum) of an objective function. \n",
    "In a convergence algorithm, such as gradient descent, the weight update formula is:\n",
    "\n",
    "$$ w_{t+1} = w_t - \\eta \\nabla L(w_t) $$\n",
    "\n",
    "Where:\n",
    "- $w_t$ is the weight at iteration $t$\n",
    "- $w_{t+1}$ is the updated weight after iteration $t$\n",
    "- $\\eta$ is the learning rate\n",
    "- $\\nabla L(w_t)$ is the gradient of the loss function with respect to $w_t$\n",
    "\n",
    "Reference: https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d6080-7b64-482a-83b2-8a7b6f8729f8",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2cf561-ffd3-4be9-be98-9c10dbea6ace",
   "metadata": {},
   "source": [
    "Multiple linear regression is an extension of simple linear regression that models the relationship between two or more independent variables (predictors) and a single dependent variable (outcome). The goal is to find the best-fit equation that describes how the dependent variable changes with variations in the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0223cd66-2bd1-44d3-8184-0901abe02767",
   "metadata": {},
   "source": [
    "The formula for multiple linear regression is:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon $$\n",
    "\n",
    "Where:\n",
    "- $y$ is the dependent variable (the value we are trying to predict)\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1, \\beta_2, \\dots, \\beta_n$ are the coefficients for each independent variable\n",
    "- $x_1, x_2, \\dots, x_n$ are the independent variables (the predictors)\n",
    "- $\\epsilon$ is the error term (residuals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762aa10-35c6-4e59-abfb-689496edd7b4",
   "metadata": {},
   "source": [
    "# Perfomance\n",
    "- R-squared\n",
    "- Adjusted R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496d281-329b-4e86-ac3d-7b73108bca1b",
   "metadata": {},
   "source": [
    "## R-squared\n",
    "R-squared (R2) is defined as a number that tells you how well the independent variable(s) in a statistical model explain the variation in the dependent variable. It ranges from 0 to 1, where 1 indicates a perfect fit of the model to the data.\n",
    "\n",
    "The formula for R-squared ($R^2$) is:\n",
    "\n",
    "$$ R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $$\n",
    "\n",
    "Where:\n",
    "- $R^2$ is the coefficient of determination\n",
    "- $y_i$ is the actual value for data point $i$\n",
    "- $\\hat{y}_i$ is the predicted value for data point $i$\n",
    "- $\\bar{y}$ is the mean of the actual values\n",
    "- $n$ is the total number of data points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5627b5b-fed9-4a02-93ef-cc8cb3b91969",
   "metadata": {},
   "source": [
    "## Adjusted R-squared\n",
    "The adjusted R-squared is a modified version of R-squared that accounts for predictors that are not significant in a regression model. In other words, the adjusted R-squared shows whether adding additional predictors improve a regression model or not.\n",
    "The formula for Adjusted R-squared ($R^2_{adj}$) is:\n",
    "\n",
    "$$ R^2_{adj} = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right) $$\n",
    "\n",
    "Where:\n",
    "- $R^2_{adj}$ is the Adjusted R-squared\n",
    "- $R^2$ is the R-squared (coefficient of determination)\n",
    "- $n$ is the total number of data points\n",
    "- $k$ is the number of independent variables (predictors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0dd750-2299-4bda-8255-08618394d0e2",
   "metadata": {},
   "source": [
    "Refernce for R-square vs Adjusted R-squared https://corporatefinanceinstitute.com/resources/data-science/adjusted-r-squared/#:~:text=Summary,adding%20value%20to%20the%20model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e469d-fa44-49b0-9b97-3dea69000800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
