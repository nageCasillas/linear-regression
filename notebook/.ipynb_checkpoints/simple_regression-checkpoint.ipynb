{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54bb5fb-88b0-4aff-be8d-70d6b51f09c2",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "## Contents\n",
    "- Description  \n",
    "- Errors\n",
    "- Cost Function\n",
    "- Convergence Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee8155-f225-498d-a51e-4180b443e134",
   "metadata": {},
   "source": [
    "## Description  \n",
    "Simple linear regression is a statistical method used to model the relationship between two variables: one independent variable (predictor) and one dependent variable (outcome). The goal is to find a linear equation, \\( y = mx + b \\), where \\( y \\) is the dependent variable, \\( x \\) is the independent variable, \\( m \\) is the slope (rate of change), and \\( b \\) is the intercept (value of \\( y \\) when \\( x = 0 \\))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636e147-f52d-4708-831a-96ed753334cc",
   "metadata": {},
   "source": [
    "## Errors\n",
    "Simple linear regression trys to make a best fit line with a minimium error.  **errors** represent the difference between the actual observed values\n",
    "and the values predicted by the regression model. The error for each data point is calculated as:\n",
    "$$ E_i = y_i - \\hat{y}_i $$\n",
    "\n",
    "Where:\n",
    "- $E_i$  is the error for the \\( i \\)-th observation,\n",
    "- $y_i$  is the actual observed value of the dependent variable,\n",
    "- $\\hat{y}_i$  is the predicted value from the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e931e6-7934-4e7f-8a2b-c11a34904c18",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "The cost function measures the difference between the predicted values of the model and the actual target values. By minimizing this cost function, we can determine the optimal values for the model‚Äôs parameters and improve its performance.\n",
    "\n",
    "The Mean Squared Error (MSE) is given by:\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "Where:\n",
    "- $y_i$ is the actual value\n",
    "- $\\hat{y}_i$ is the predicted value\n",
    "- $n$ is the total number of data points\n",
    "\n",
    "Reference: https://medium.com/@yennhi95zz/3-understanding-the-cost-function-in-linear-regression-for-machine-learning-beginners-ec9edeecbdde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12abd447-c326-4846-b2fb-15b5efbee898",
   "metadata": {},
   "source": [
    "## Convergence Algorithm\n",
    "Gradient Descent is an iterative optimization algorithm that tries to find the optimum value (Minimum/Maximum) of an objective function. \n",
    "In a convergence algorithm, such as gradient descent, the weight update formula is:\n",
    "\n",
    "$$ w_{t+1} = w_t - \\eta \\nabla L(w_t) $$\n",
    "\n",
    "Where:\n",
    "- $w_t$ is the weight at iteration $t$\n",
    "- $w_{t+1}$ is the updated weight after iteration $t$\n",
    "- $\\eta$ is the learning rate\n",
    "- $\\nabla L(w_t)$ is the gradient of the loss function with respect to $w_t$\n",
    "\n",
    "Reference: https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d6080-7b64-482a-83b2-8a7b6f8729f8",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2cf561-ffd3-4be9-be98-9c10dbea6ace",
   "metadata": {},
   "source": [
    "Multiple linear regression is an extension of simple linear regression that models the relationship between two or more independent variables (predictors) and a single dependent variable (outcome). The goal is to find the best-fit equation that describes how the dependent variable changes with variations in the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0223cd66-2bd1-44d3-8184-0901abe02767",
   "metadata": {},
   "source": [
    "The formula for multiple linear regression is:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon $$\n",
    "\n",
    "Where:\n",
    "- $y$ is the dependent variable (the value we are trying to predict)\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1, \\beta_2, \\dots, \\beta_n$ are the coefficients for each independent variable\n",
    "- $x_1, x_2, \\dots, x_n$ are the independent variables (the predictors)\n",
    "- $\\epsilon$ is the error term (residuals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762aa10-35c6-4e59-abfb-689496edd7b4",
   "metadata": {},
   "source": [
    "# Perfomance\n",
    "- R-squared\n",
    "- Adjusted R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496d281-329b-4e86-ac3d-7b73108bca1b",
   "metadata": {},
   "source": [
    "## R-squared\n",
    "R-squared (R2) is defined as a number that tells you how well the independent variable(s) in a statistical model explain the variation in the dependent variable. It ranges from 0 to 1, where 1 indicates a perfect fit of the model to the data.\n",
    "\n",
    "The formula for R-squared ($R^2$) is:\n",
    "\n",
    "$$ R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $$\n",
    "\n",
    "Where:\n",
    "- $R^2$ is the coefficient of determination\n",
    "- $y_i$ is the actual value for data point $i$\n",
    "- $\\hat{y}_i$ is the predicted value for data point $i$\n",
    "- $\\bar{y}$ is the mean of the actual values\n",
    "- $n$ is the total number of data points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5627b5b-fed9-4a02-93ef-cc8cb3b91969",
   "metadata": {},
   "source": [
    "## Adjusted R-squared\n",
    "The adjusted R-squared is a modified version of R-squared that accounts for predictors that are not significant in a regression model. In other words, the adjusted R-squared shows whether adding additional predictors improve a regression model or not.\n",
    "The formula for Adjusted R-squared ($R^2_{adj}$) is:\n",
    "\n",
    "$$ R^2_{adj} = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right) $$\n",
    "\n",
    "Where:\n",
    "- $R^2_{adj}$ is the Adjusted R-squared\n",
    "- $R^2$ is the R-squared (coefficient of determination)\n",
    "- $n$ is the total number of data points\n",
    "- $k$ is the number of independent variables (predictors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0dd750-2299-4bda-8255-08618394d0e2",
   "metadata": {},
   "source": [
    "Refernce for R-square vs Adjusted R-squared https://corporatefinanceinstitute.com/resources/data-science/adjusted-r-squared/#:~:text=Summary,adding%20value%20to%20the%20model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a686427-83d7-46d7-b911-fbdf3eb92f64",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f27d8-19d9-43a8-8b50-c6db99838895",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis where the relationship between the independent variable(s) and the dependent variable is modeled as a polynomial of a certain degree. Unlike linear regression, which assumes a straight-line relationship, polynomial regression can capture more complex, curved patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbf12c4-fd1c-499c-bfe4-fa86d8899b50",
   "metadata": {},
   "source": [
    "The formula for polynomial regression (of degree $d$) is:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\dots + \\beta_d x^d + \\epsilon $$\n",
    "\n",
    "Where:\n",
    "- $y$ is the dependent variable (the value we are trying to predict)\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1, \\beta_2, \\dots, \\beta_d$ are the coefficients for each degree of the independent variable\n",
    "- $x$ is the independent variable (the predictor)\n",
    "- $d$ is the degree of the polynomial\n",
    "- $\\epsilon$ is the error term (residuals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1609f2-4e5f-4a4e-a01b-cd3972befe20",
   "metadata": {},
   "source": [
    "By increasing the degree d, the model can fit more complex, non-linear relationships between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3226a95-c8ba-4f19-ba03-5489e0430972",
   "metadata": {},
   "source": [
    "# Ridge regression\n",
    "Ridge regression is a type of linear regression that adds a regularization term to the cost function to prevent overfitting. It is particularly useful when the model has many features or when multicollinearity exists, which can lead to large and unstable coefficients in standard linear regression.\n",
    "\n",
    "The cost function for ridge regression includes a penalty term, which is the sum of the squared values of the coefficients:\n",
    "$$ J(\\beta) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 $$\n",
    "\n",
    "Where:\n",
    "- $J(\\beta)$ is the cost function\n",
    "- $y_i$ is the actual value for data point $i$\n",
    "- $\\hat{y}_i$ is the predicted value for data point $i$\n",
    "- $\\lambda$ is the regularization parameter (controls the strength of regularization)\n",
    "- $\\beta_j$ is the coefficient of the $j$-th feature\n",
    "- $n$ is the total number of data points\n",
    "- $p$ is the total number of features (independent variables)\n",
    "\n",
    "The regularization term shrinks the coefficients, reducing model complexity and helping prevent overfitting, especially when there are highly correlated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf72cb-1d15-4dc0-ae60-a19cf6ddbe9b",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "Lasso regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that adds an ùêø1.\n",
    "L1 regularization term to the cost function. Like ridge regression, it helps prevent overfitting, but lasso has an additional feature: it can reduce some coefficients to exactly zero, effectively performing feature selection by eliminating irrelevant predictors.\n",
    "The cost function for Lasso Regression is:\n",
    "\n",
    "$$ J(\\beta) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| $$\n",
    "\n",
    "Where:\n",
    "- $J(\\beta)$ is the cost function\n",
    "- $y_i$ is the actual value for data point $i$\n",
    "- $\\hat{y}_i$ is the predicted value for data point $i$\n",
    "- $\\lambda$ is the regularization parameter (controls the strength of regularization)\n",
    "- $\\beta_j$ is the coefficient of the $j$-th feature\n",
    "- $|\\beta_j|$ is the absolute value of the coefficient $\\beta_j$ (L1 regularization)\n",
    "- $n$ is the total number of data points\n",
    "- $p$ is the total number of features (independent variables)\n",
    "\n",
    "Lasso‚Äôs ability to shrink some coefficients to zero makes it especially useful for sparse models with many irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce2585-a7f0-4572-91bc-3949bd29eb0c",
   "metadata": {},
   "source": [
    "# Elastic Net Regrssion\n",
    "Elastic Net regression is a linear regression model that combines the regularization techniques of both ridge regression (L2 regularization) and lasso regression (L1 regularization). It helps address the limitations of both methods by balancing their strengths, making it useful for models with high-dimensional data or multicollinearity. \n",
    "\n",
    "The cost function for Elastic Net Regression is:\n",
    "\n",
    "$$ J(\\beta) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\left( \\alpha \\sum_{j=1}^{p} |\\beta_j| + (1 - \\alpha) \\sum_{j=1}^{p} \\beta_j^2 \\right) $$\n",
    "\n",
    "Where:\n",
    "- $J(\\beta)$ is the cost function\n",
    "- $y_i$ is the actual value for data point $i$\n",
    "- $\\hat{y}_i$ is the predicted value for data point $i$\n",
    "- $\\lambda$ is the regularization parameter (controls the strength of regularization)\n",
    "- $\\alpha$ controls the balance between L1 (Lasso) and L2 (Ridge) regularization\n",
    "- $|\\beta_j|$ is the absolute value of the coefficient $\\beta_j$ (L1 regularization)\n",
    "- $\\beta_j^2$ is the square of the coefficient $\\beta_j$ (L2 regularization)\n",
    "- $n$ is the total number of data points\n",
    "- $p$ is the total number of features (independent variables)\n",
    "\n",
    "Elastic Net can perform both feature selection (like lasso) and shrinkage (like ridge), making it particularly powerful for models with many correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647b8a3-1788-4c25-ad5c-ab59db8f56ca",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "Cross-validation is a technique used to assess the performance of a machine learning model by testing it on different subsets of the data. It helps to ensure that the model generalizes well to unseen data and avoids issues like overfitting or underfitting.\n",
    "Common Technique of Cross Validation:\n",
    "- Leave one out CV\n",
    "- Leave P out CV\n",
    "- K fold CV\n",
    "- Stratified K fold CV\n",
    "- Time Series cv\n",
    "This technique provides a more reliable estimate of model performance than a single train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f005d-136b-4f63-b18e-2a170a42d4b8",
   "metadata": {},
   "source": [
    "## Leave one out CV\n",
    "In this method, the model is trained on all but one data point and tested on the single remaining point. This is repeated for each data point in the dataset, so for n data points, there are n training/testing cycles.\n",
    "This method is computationally expensive for large datasets but provides an exhaustive evaluation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ccbaf-b2f1-48d5-836c-616769eeff2a",
   "metadata": {},
   "source": [
    "## Leave P out CV\n",
    "A generalization of LOO CV, where instead of leaving out one data point, P data points are left out for testing, and the model is trained on the remaining n‚àíP data points. This process is repeated for all possible combinations of P data points. Like LOO CV, LPO CV is computationally intensive and is typically used only for small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b8be47-11ce-4570-8ec5-b31c9883cf17",
   "metadata": {},
   "source": [
    "## K fold CV\n",
    "The dataset is divided into k equal-sized folds. The model is trained on k‚àí1 folds and tested on the remaining fold. This process is repeated k times, with each fold serving as the test set once. It is more computationally efficient than LOO CV and provides a more reliable performance estimate than a single train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3667c7-c8db-4482-9af5-4c70382d08e2",
   "metadata": {},
   "source": [
    "## Stratified K fold CV\n",
    "Similar to K-fold CV, but it ensures that each fold has approximately the same distribution of the target variable (e.g., in classification problems, the class proportions in each fold are similar to the original dataset). This is particularly useful when the dataset is imbalanced, ensuring that each fold is representative of the overall data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e843ef68-bd3a-48c6-9cb8-4617433a2864",
   "metadata": {},
   "source": [
    "## Time Series CV\n",
    "In time series data, the order of observations is crucial, so traditional K-fold CV cannot be applied. Instead, time series CV involves training the model on past data and testing it on future data. One approach is the rolling window or expanding window method, where the training set grows with each iteration, and the test set is always forward-looking in time. This method respects the temporal order of the data and avoids data leakage from future to past."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
